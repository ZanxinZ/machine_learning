- 多变量线性回归(Liner Regression with Multiple Variables

- 多变量梯度下降

  对于某一个输入，有以下的形式
  $$
  输入：x=\begin{bmatrix}x_1\\x_2\\x_3\\.\\.\\x_n \end{bmatrix}
  $$
   【假设 h（hypothesis)】：猜测函数
  $$
  h_\theta(x)=\theta^TX=\theta_1x_1+\theta_2x_2+\theta_3x_3+...+\theta_nx_n
  $$
   最终任务：用这个 h 去回归 实际函数 f ，而损失（代价） J 即是描述 h 和 f 的差距
  $$
  J(\theta_0,\theta_1...\theta_n)=\frac 1{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{i})^2
  $$

- 特征和多项式回归

  二次方模型：
  $$
  h_\theta(x)=\theta_0+\theta_1x_1+\theta_0x_2^2
  $$
  三次方模型：
  $$
  h_\theta(x)=\theta_0+\theta_1x_1+\theta_0x_2^2+\theta_3x_3^3
  $$

- 正规方程方法(某些线性回归问题比较合适)
  $$
  J(\theta)=a\theta^2+b\theta+c
  $$
  求解方程：
  $$
  \theta = (X^TX)^{-1}X^Ty
  $$
  找出使得损失函数（代价函数）最小的的参数：
  $$
  \frac \partial {\partial\theta_j}J(\theta_j)=0
  $$

|          梯度下降           |                           正规方程                           |
| :-------------------------: | :----------------------------------------------------------: |
|       需要选择学习率        |                         不需要学习率                         |
|        需要多次迭代         |                         一次运算完成                         |
| 当特征数量n大时也能较好适用 | 需要矩阵求逆，时间复杂度$O(n^3)$，当 n 较大时运算代价大,n小于10000还是可以接受 |
|    适用与各种类型地模型     |             只适用于线性模型，不适合逻辑回归模型             |